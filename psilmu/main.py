import base64
import cv2
import numpy as np
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import os

# 1. FastAPI ì•± ì„¤ì •
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. MediaPipe Face Landmarker ì„¤ì •
# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(BASE_DIR, "face_landmarker.task")

if not os.path.exists(model_path):
    raise FileNotFoundError(f"'{model_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. main.pyì™€ ê°™ì€ í´ë”ì— ìœ„ì¹˜ì‹œì¼œì£¼ì„¸ìš”.")


def get_blendshape_map(blendshapes):
    return {b.category_name: b.score for b in blendshapes}

def matrix_to_euler(matrix):
    R = matrix[:3, :3]
    sy = np.sqrt(R[0,0] ** 2 + R[1,0] ** 2)
    singular = sy < 1e-6
    if not singular:
        pitch = np.arctan2(R[2,1], R[2,2])
        yaw = np.arctan2(-R[2,0], sy)
        roll = np.arctan2(R[1,0], R[0,0])
    else:
        pitch = np.arctan2(-R[1,2], R[1,1])
        yaw = np.arctan2(-R[2,0], sy)
        roll = 0
        
    return pitch, yaw, roll

# smile detection
def smile(blendshapes):
    m = get_blendshape_map(blendshapes)
    left=m.get("mouthSmileLeft", 0.0)
    right=m.get("mouthSmileRight", 0.0)
    # ëˆˆê°€ ë¯¸ì†Œ
    squint_l = m.get("eyeSquintLeft", 0.0)
    squint_r = m.get("eyeSquintRight", 0.0)
    
    base_smile = (left + right) / 2.0
    duchenne_bonus = (squint_l + squint_r) / 2.0 * 0.5
    score = np.clip(base_smile + duchenne_bonus, 0.0, 1.0)
    
    return float(score)
    
# head pose detection 
DOWN_TH = -0.25    # ì´ ê°’ë³´ë‹¤ ì‘ì•„ì§€ë©´ 'ì•„ë˜ë¡œ ìˆ™ì˜€ë‹¤'ê³  ë´„ ì¼ë‹¨ í…ŒìŠ¤íŠ¸í•˜ë©´ì„œ ì„¸íŒ…í•¨
UP_TH   = -0.15    # ì´ ê°’ ì´ìƒìœ¼ë¡œ ë‹¤ì‹œ ì˜¬ë¼ì˜¤ë©´ 'ì›ìœ„ì¹˜'ë¡œ ë´„
MAX_NOD_FRAMES = 240  # 30í”„ë ˆì„ ì•ˆ(ëŒ€ì¶© 1ì´ˆ ì•ˆ)ì— ë³µê·€í•˜ë©´ ì§„ì§œ ë„ë•ì„

# ì§€ê¸ˆì€ ë‹¨ìˆœíˆ 1ì´ˆì´ë‚´ ê³ ê°œì˜ ìœ„ì•„ë˜ ì›€ì§ì„ìœ¼ë¡œë§Œ íŒë‹¨í•˜ëŠ”ë° ê³ ê°œë¥¼ ë„ë•ì¸ê±´ì§€ ê·¸ëƒ¥ ê³ ê°œë¥¼ ìˆ™ì¸ê±´ì§€ êµ¬ë¶„ì´ ì•ˆë¨
def head_pose_matrix(result):
    if not result.facial_transformation_matrixes:
        return None
    
    m = result.facial_transformation_matrixes[0]
    matrix4 = np.array(m, dtype = np.float32).reshape(4,4).T
    
    pitch,yaw,roll = matrix_to_euler(matrix4)
    return pitch, yaw, roll

# eye contact detection
def eye_contact(blendshapes, max_dev=0.8):
    m = get_blendshape_map(blendshapes)

    # ì™¼ìª½ ëˆˆ ë°©í–¥ ë²¡í„°
    lh = m.get("eyeLookOutLeft", 0.0) - m.get("eyeLookInLeft", 0.0)
    lv = m.get("eyeLookUpLeft", 0.0)  - m.get("eyeLookDownLeft", 0.0)
    left_mag = np.sqrt(lh * lh + lv * lv)

    # ì˜¤ë¥¸ìª½ ëˆˆ ë°©í–¥ ë²¡í„°
    rh = m.get("eyeLookOutRight", 0.0) - m.get("eyeLookInRight", 0.0)
    rv = m.get("eyeLookUpRight", 0.0)  - m.get("eyeLookDownRight", 0.0)
    right_mag = np.sqrt(rh * rh + rv * rv)

    # ë‘ ëˆˆ í‰ê·  "í¸í–¥ í¬ê¸°"
    dev = (left_mag + right_mag) / 2.0  # 0 = ì •ë©´, ì»¤ì§ˆìˆ˜ë¡ ë”´ ë°

    # devê°€ max_dev ì´ìƒì´ë©´ "ì™„ì „ ë”´ ë° ë³¸ë‹¤"ë¡œ ë³´ê³  1ë¡œ saturate
    dev_norm = np.clip(dev / max_dev, 0.0, 1.0)

    # eye-contact ì ìˆ˜ = 1 - deviation
    score = 1.0 - dev_norm
    
    return float(score)

# web
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print(">>> Client Connected")

    # BaseOptions: ëª¨ë¸ íŒŒì¼ ì§€ì •
    base_options = python.BaseOptions(model_asset_path=model_path)

    # FaceLandmarkerOptions: ì‹¤í–‰ ëª¨ë“œ ë° ì¶œë ¥ ì„¤ì •
    options = vision.FaceLandmarkerOptions(
        base_options=base_options,
        running_mode=vision.RunningMode.VIDEO, # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ
        min_face_detection_confidence=0.5, # ì–¼êµ´ ê°ì§€ ìµœì†Œ ì‹ ë¢°ë„
        min_face_presence_confidence=0.5, # ì–¼êµ´ ì¡´ì¬ ìµœì†Œ ì‹ ë¢°ë„
        min_tracking_confidence=0.5, # ì¶”ì  ìµœì†Œ ì‹ ë¢°ë„
        output_face_blendshapes=True, # í‘œì • ë¶„ì„(ë¸”ë Œë“œì‰ì´í”„) í•„ìš” ì‹œ True
        output_facial_transformation_matrixes=True,
        num_faces=1,
    )

    # Landmarker ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    detector = vision.FaceLandmarker.create_from_options(options)
    
    frame_id = 0

    nod_state = "IDLE"
    nod_count = 0
    nod_start_frame = 0
    try:
        while True:
            # 1. ë°ì´í„° ìˆ˜ì‹ 
            data = await websocket.receive_text()

            try:
                
                # 2. Base64 ë””ì½”ë”©
                if ',' in data:
                    data = data.split(',')[1]
                
                image_bytes = base64.b64decode(data)
                np_arr = np.frombuffer(image_bytes, np.uint8)
                
                # BGR ì´ë¯¸ì§€ (OpenCV í¬ë§·)
                frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

                if frame is None:
                    continue

                # 3. MediaPipe Image ê°ì²´ë¡œ ë³€í™˜
                # OpenCVëŠ” BGRì„ ì“°ì§€ë§Œ, MediaPipeëŠ” RGBë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)

                # í”„ë ˆì„ íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚° (ms ë‹¨ìœ„)
                frame_id += 1

                # --- ì¸í„°ë·° í†µê³„ìš© ëˆ„ì  ë³€ìˆ˜ ---
                face_frame_count = 0          # ì–¼êµ´ì´ ì¡íŒ í”„ë ˆì„ ìˆ˜
                smile_sum = 0.0               # ë¯¸ì†Œ ì ìˆ˜ ì´í•©
                eye_on_frames = 0             # 'ì•„ì´ì»¨íƒ'ì´ë¼ê³  ê°„ì£¼í•œ í”„ë ˆì„ ìˆ˜

                SMILE_TH = 0.15               # ë¯¸ì†Œ ê¸°ì¤€
                EYE_TH = 0.6                  # ì•„ì´ì»¨íƒ ê¸°ì¤€
                            
                if frame_id % 2 != 0:
                    continue  # ì´ í”„ë ˆì„ì€ ì¶”ë¡  ì•ˆ í•˜ê³  ë‹¤ìŒìœ¼ë¡œ
                
                # 4. ì¶”ë¡  ì‹¤í–‰
                detection_result = detector.detect_for_video(mp_image,frame_id)

                # 5. ê²°ê³¼ ì¶œë ¥
                if detection_result.face_landmarks:
                    # ì²« ë²ˆì§¸ ì–¼êµ´ì˜ ëœë“œë§ˆí¬ ê°€ì ¸ì˜¤ê¸°
                    face_landmarks = detection_result.face_landmarks[0]
                    # ì–¼êµ´ í‘œì •(ë¸”ë Œë“œì‰ì´í”„) ê°€ì ¸ì˜¤ê¸°
                    blend = detection_result.face_blendshapes[0]
                    
                    smile_score = smile(blend)# ë¯¸ì†Œ ì ìˆ˜
                    pitch, yaw, roll = head_pose_matrix(detection_result)  # ë¨¸ë¦¬ ìì„¸
                    eye_contact_score = eye_contact(blend)  # ì•„ì´ ì»¨íƒíŠ¸ ì ìˆ˜

                    # ì¸í„°ë·° í†µê³„ ëˆ„ì 
                    face_frame_count += 1
                    smile_sum += smile_score  # 0~1 -> 0~100 ìŠ¤ì¼€ì¼ë¡œ ëˆ„ì 
                    
                    if eye_contact_score > EYE_TH:
                        eye_on_frames += 1
                    
                    if nod_state == "IDLE":
                        # ê¸°ì¤€ë³´ë‹¤ ì¶©ë¶„íˆ ì•„ë˜ë¡œ(ê³ ê°œ ìˆ™ì„)
                        if pitch < DOWN_TH:
                            nod_state = "DOWN"
                            nod_start_frame = frame_id

                    elif nod_state == "DOWN":
                        # ë‹¤ì‹œ ìœ„ë¡œ ì˜¬ë¼ì™€ì„œ ê±°ì˜ ê¸°ë³¸ìì„¸ ê·¼ì²˜ë¡œ ë³µê·€
                        if pitch > UP_TH:
                            # ë„ˆë¬´ ëŠë¦° ì›€ì§ì„ì€ ê·¸ëƒ¥ 'ìì„¸ ë°”ê¿ˆ'ìœ¼ë¡œ ë³´ê³  ë²„ë¦¼
                            if frame_id - nod_start_frame <= MAX_NOD_FRAMES:
                                nod_count += 1
                                print(f"ğŸ‘† Nod detected! total={nod_count}")
                            nod_state = "IDLE"
                    
                    if frame_id % 10 == 0:
                        # ê²°ê³¼ ì¶œë ¥ pitch ìœ„ì•„ë˜ë¡œ 0.3 eye contact > 0.6
                        print(f"[Face Detected] Smile: {smile_score:.2f}, Head Pose (Pitch: {pitch:.2f}, Yaw: {yaw:.2f}, Roll: {roll:.2f}), Eye Contact: {eye_contact_score:.2f}")
                    
                else:
                    print("[No Face] Waiting for user...")

            except Exception as e:
                print(f"Error processing frame: {e}")

    except WebSocketDisconnect:
        avg_smile = smile_sum / face_frame_count * 100
        eye_ratio = eye_on_frames / face_frame_count
        
        print(f">>> Interview Summary: Average Smile Score: {avg_smile:.2f}, Eye Contact Ratio: {eye_ratio:.2f}, Total Nods: {nod_count}")
        print(">>> Client Disconnected")
    except Exception as e:
        print(f">>> Connection Error: {e}")